# -*- coding: utf-8 -*-
"""
æ€§èƒ½ä¼˜åŒ–æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯HF Spacesç¯å¢ƒä¸­çš„æ€§èƒ½ä¼˜åŒ–æ•ˆæœ
"""

import time
import logging
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict
import json
from datetime import datetime

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PerformanceOptimizationTest:
    """æ€§èƒ½ä¼˜åŒ–æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.test_stocks = [
            '000001', '000002', '000858', '002415', '600036',
            '600519', '000858', '002594', '300059', '600276'
        ]
        self.results = {}
        
    def test_database_performance(self):
        """æµ‹è¯•æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–"""
        logger.info("ğŸ” æµ‹è¯•æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–...")
        
        try:
            from database import batch_get_stock_info, batch_get_realtime_data, get_session
            from sqlalchemy import text
            
            # æµ‹è¯•æ‰¹é‡æŸ¥è¯¢æ€§èƒ½
            start_time = time.time()
            batch_info = batch_get_stock_info(self.test_stocks)
            batch_query_time = time.time() - start_time
            
            # æµ‹è¯•å•ä¸ªæŸ¥è¯¢æ€§èƒ½ï¼ˆå¯¹æ¯”ï¼‰
            start_time = time.time()
            session = get_session()
            for _ in range(5):  # æµ‹è¯•5æ¬¡å•ä¸ªæŸ¥è¯¢
                session.execute(text("SELECT 1"))
            session.close()
            single_query_time = time.time() - start_time
            
            self.results['database'] = {
                'batch_query_time': batch_query_time,
                'batch_stocks_count': len(batch_info),
                'single_query_time': single_query_time,
                'optimization_effective': batch_query_time < single_query_time * 2
            }
            
            logger.info(f"âœ… æ•°æ®åº“æµ‹è¯•å®Œæˆ: æ‰¹é‡æŸ¥è¯¢{len(batch_info)}åªè‚¡ç¥¨è€—æ—¶{batch_query_time:.2f}ç§’")
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“æµ‹è¯•å¤±è´¥: {e}")
            self.results['database'] = {'error': str(e)}
    
    def test_cache_performance(self):
        """æµ‹è¯•ç¼“å­˜æ€§èƒ½ä¼˜åŒ–"""
        logger.info("ğŸ” æµ‹è¯•ç¼“å­˜æ€§èƒ½ä¼˜åŒ–...")
        
        try:
            from advanced_cache_manager import AdvancedCacheManager
            
            cache_manager = AdvancedCacheManager()
            
            # æµ‹è¯•ç¼“å­˜è®¾ç½®å’Œè·å–
            test_data = {'test': 'data', 'timestamp': time.time()}
            
            # è®¾ç½®ç¼“å­˜
            start_time = time.time()
            cache_manager.set('test_data', test_data, stock_code='TEST001')
            set_time = time.time() - start_time
            
            # è·å–ç¼“å­˜
            start_time = time.time()
            cached_data = cache_manager.get('test_data', stock_code='TEST001')
            get_time = time.time() - start_time
            
            self.results['cache'] = {
                'set_time': set_time,
                'get_time': get_time,
                'cache_hit': cached_data is not None,
                'data_integrity': cached_data == test_data if cached_data else False
            }
            
            logger.info(f"âœ… ç¼“å­˜æµ‹è¯•å®Œæˆ: è®¾ç½®è€—æ—¶{set_time:.4f}ç§’, è·å–è€—æ—¶{get_time:.4f}ç§’")
            
        except Exception as e:
            logger.error(f"âŒ ç¼“å­˜æµ‹è¯•å¤±è´¥: {e}")
            self.results['cache'] = {'error': str(e)}
    
    def test_concurrent_analysis(self):
        """æµ‹è¯•å¹¶å‘åˆ†ææ€§èƒ½"""
        logger.info("ğŸ” æµ‹è¯•å¹¶å‘åˆ†ææ€§èƒ½...")
        
        try:
            from stock_analyzer import StockAnalyzer
            
            analyzer = StockAnalyzer()
            
            # ä¸²è¡Œåˆ†ææµ‹è¯•
            start_time = time.time()
            serial_results = []
            for stock_code in self.test_stocks[:5]:  # æµ‹è¯•5åªè‚¡ç¥¨
                try:
                    result = analyzer.quick_analyze_stock(stock_code, timeout=30)
                    serial_results.append(result)
                except Exception as e:
                    logger.warning(f"ä¸²è¡Œåˆ†æ {stock_code} å¤±è´¥: {e}")
            serial_time = time.time() - start_time
            
            # å¹¶å‘åˆ†ææµ‹è¯•
            start_time = time.time()
            concurrent_results = []
            with ThreadPoolExecutor(max_workers=4) as executor:
                future_to_stock = {
                    executor.submit(analyzer._safe_quick_analyze, stock_code, 'A', 0): stock_code 
                    for stock_code in self.test_stocks[:5]
                }
                
                for future in as_completed(future_to_stock, timeout=60):
                    try:
                        result = future.result()
                        if result:
                            concurrent_results.append(result)
                    except Exception as e:
                        logger.warning(f"å¹¶å‘åˆ†æå¤±è´¥: {e}")
            
            concurrent_time = time.time() - start_time
            
            self.results['concurrent_analysis'] = {
                'serial_time': serial_time,
                'serial_count': len(serial_results),
                'concurrent_time': concurrent_time,
                'concurrent_count': len(concurrent_results),
                'speedup_ratio': serial_time / concurrent_time if concurrent_time > 0 else 0,
                'optimization_effective': concurrent_time < serial_time
            }
            
            logger.info(f"âœ… å¹¶å‘åˆ†ææµ‹è¯•å®Œæˆ: ä¸²è¡Œ{serial_time:.2f}ç§’ vs å¹¶å‘{concurrent_time:.2f}ç§’")
            
        except Exception as e:
            logger.error(f"âŒ å¹¶å‘åˆ†ææµ‹è¯•å¤±è´¥: {e}")
            self.results['concurrent_analysis'] = {'error': str(e)}
    
    def test_timeout_configuration(self):
        """æµ‹è¯•è¶…æ—¶é…ç½®"""
        logger.info("ğŸ” æµ‹è¯•è¶…æ—¶é…ç½®...")
        
        try:
            from hf_spaces_performance_config import get_hf_timeout, is_hf_feature_enabled
            
            # æ£€æŸ¥è¶…æ—¶é…ç½®
            api_timeout = get_hf_timeout('api')
            analysis_timeout = get_hf_timeout('analysis')
            data_fetch_timeout = get_hf_timeout('data_fetch')
            
            # æ£€æŸ¥åŠŸèƒ½å¼€å…³
            ai_analysis_enabled = is_hf_feature_enabled('ai_analysis')
            complex_indicators_enabled = is_hf_feature_enabled('complex_indicators')
            
            self.results['timeout_config'] = {
                'api_timeout': api_timeout,
                'analysis_timeout': analysis_timeout,
                'data_fetch_timeout': data_fetch_timeout,
                'ai_analysis_enabled': ai_analysis_enabled,
                'complex_indicators_enabled': complex_indicators_enabled,
                'timeout_extended': api_timeout >= 180
            }
            
            logger.info(f"âœ… è¶…æ—¶é…ç½®æµ‹è¯•å®Œæˆ: APIè¶…æ—¶{api_timeout}ç§’, åˆ†æè¶…æ—¶{analysis_timeout}ç§’")
            
        except Exception as e:
            logger.error(f"âŒ è¶…æ—¶é…ç½®æµ‹è¯•å¤±è´¥: {e}")
            self.results['timeout_config'] = {'error': str(e)}
    
    def test_performance_monitoring(self):
        """æµ‹è¯•æ€§èƒ½ç›‘æ§"""
        logger.info("ğŸ” æµ‹è¯•æ€§èƒ½ç›‘æ§...")
        
        try:
            from performance_monitor import get_hf_spaces_performance_report, performance_monitor
            
            # æ¨¡æ‹Ÿä¸€äº›æ€§èƒ½æ•°æ®
            performance_monitor.record_api_call(1.5, success=True)
            performance_monitor.record_cache_hit(0.01)
            performance_monitor.record_db_query(0.1, success=True)
            
            # è·å–æ€§èƒ½æŠ¥å‘Š
            report = get_hf_spaces_performance_report()
            
            self.results['performance_monitoring'] = {
                'report_generated': report is not None,
                'has_recommendations': len(report.get('recommendations', [])) >= 0,
                'cache_hit_rate': report.get('cache_hit_rate', 0),
                'avg_api_time': report.get('avg_api_time', 0)
            }
            
            logger.info(f"âœ… æ€§èƒ½ç›‘æ§æµ‹è¯•å®Œæˆ: ç¼“å­˜å‘½ä¸­ç‡{report.get('cache_hit_rate', 0):.1f}%")
            
        except Exception as e:
            logger.error(f"âŒ æ€§èƒ½ç›‘æ§æµ‹è¯•å¤±è´¥: {e}")
            self.results['performance_monitoring'] = {'error': str(e)}
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹æ€§èƒ½ä¼˜åŒ–æµ‹è¯•...")
        
        start_time = time.time()
        
        # è¿è¡Œå„é¡¹æµ‹è¯•
        self.test_database_performance()
        self.test_cache_performance()
        self.test_concurrent_analysis()
        self.test_timeout_configuration()
        self.test_performance_monitoring()
        
        total_time = time.time() - start_time
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        self.results['test_summary'] = {
            'total_test_time': total_time,
            'test_timestamp': datetime.now().isoformat(),
            'tests_passed': sum(1 for result in self.results.values() 
                              if isinstance(result, dict) and 'error' not in result),
            'tests_failed': sum(1 for result in self.results.values() 
                              if isinstance(result, dict) and 'error' in result)
        }
        
        logger.info(f"âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
        
        return self.results
    
    def generate_report(self, filepath: str = None):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        if filepath is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filepath = f"performance_test_report_{timestamp}.json"
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ğŸ“Š æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æµ‹è¯•æŠ¥å‘Šå¤±è´¥: {e}")
            return None


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ HF Spaces æ€§èƒ½ä¼˜åŒ–æµ‹è¯•")
    print("=" * 50)
    
    # åˆ›å»ºæµ‹è¯•å®ä¾‹
    test = PerformanceOptimizationTest()
    
    # è¿è¡Œæµ‹è¯•
    results = test.run_all_tests()
    
    # ç”ŸæˆæŠ¥å‘Š
    report_file = test.generate_report()
    
    # æ‰“å°æ‘˜è¦
    print("\nğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦:")
    print("=" * 50)
    
    summary = results.get('test_summary', {})
    print(f"æ€»æµ‹è¯•æ—¶é—´: {summary.get('total_test_time', 0):.2f}ç§’")
    print(f"é€šè¿‡æµ‹è¯•: {summary.get('tests_passed', 0)}")
    print(f"å¤±è´¥æµ‹è¯•: {summary.get('tests_failed', 0)}")
    
    if report_file:
        print(f"è¯¦ç»†æŠ¥å‘Š: {report_file}")
    
    print("\nğŸ¯ ä¼˜åŒ–æ•ˆæœè¯„ä¼°:")
    print("=" * 50)
    
    # è¯„ä¼°å„é¡¹ä¼˜åŒ–æ•ˆæœ
    if 'database' in results and 'optimization_effective' in results['database']:
        status = "âœ…" if results['database']['optimization_effective'] else "âŒ"
        print(f"{status} æ•°æ®åº“ä¼˜åŒ–: {'æœ‰æ•ˆ' if results['database']['optimization_effective'] else 'éœ€è¦æ”¹è¿›'}")
    
    if 'concurrent_analysis' in results and 'optimization_effective' in results['concurrent_analysis']:
        status = "âœ…" if results['concurrent_analysis']['optimization_effective'] else "âŒ"
        speedup = results['concurrent_analysis'].get('speedup_ratio', 0)
        print(f"{status} å¹¶å‘åˆ†æä¼˜åŒ–: {'æœ‰æ•ˆ' if results['concurrent_analysis']['optimization_effective'] else 'éœ€è¦æ”¹è¿›'} (åŠ é€Ÿæ¯”: {speedup:.2f}x)")
    
    if 'timeout_config' in results and 'timeout_extended' in results['timeout_config']:
        status = "âœ…" if results['timeout_config']['timeout_extended'] else "âŒ"
        print(f"{status} è¶…æ—¶é…ç½®: {'å·²å»¶é•¿åˆ°180ç§’' if results['timeout_config']['timeout_extended'] else 'éœ€è¦è°ƒæ•´'}")


if __name__ == "__main__":
    main()
